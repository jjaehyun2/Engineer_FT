{
    "model_type": "decoder_only",
    "tokenizer": {
        "type": "sentencepiece",
        "vocab_size": 50000,
        "training": {
            "input_files": [
                "data/filtered/all_filtered.jsonl"
            ],
            "training_samples": 1000000,
            "model_prefix": "models/tokenizer/code_tokenizer"
        }
    },
    "model": {
        "model_size": "base",
        "hidden_size": 768,
        "num_hidden_layers": 12,
        "num_attention_heads": 12,
        "intermediate_size": 3072,
        "hidden_activation": "gelu",
        "max_position_embeddings": 2048,
        "layer_norm_eps": 1e-12,
        "dropout_prob": 0.1,
        "attention_dropout_prob": 0.1,
        "initializer_range": 0.02
    },
    "training": {
        "batch_size": 32,
        "micro_batch_size": 4,
        "learning_rate": 5e-5,
        "weight_decay": 0.01,
        "adam_beta1": 0.9,
        "adam_beta2": 0.999,
        "adam_epsilon": 1e-8,
        "max_grad_norm": 1.0,
        "num_train_epochs": 3,
        "warmup_steps": 10000,
        "optimizer": "adamw",
        "lr_scheduler": "cosine",
        "gradient_accumulation_steps": 8,
        "evaluation_strategy": "steps",
        "eval_steps": 500,
        "save_steps": 1000,
        "logging_steps": 100
    },
    "data": {
        "train_file": "data/filtered/train.jsonl",
        "validation_file": "data/filtered/validation.jsonl",
        "test_file": "data/filtered/test.jsonl",
        "max_seq_length": 1024,
        "preprocessing_num_workers": 4,
        "shuffle_buffer_size": 10000,
        "train_val_test_split": [
            0.98,
            0.01,
            0.01
        ]
    },
    "evaluation": {
        "metrics": [
            "perplexity",
            "code_bleu",
            "exact_match"
        ],
        "generation_max_length": 512,
        "generation_num_beams": 5
    },
    "checkpoint": {
        "save_directory": "models/checkpoints",
        "save_total_limit": 3
    },
    "inference": {
        "temperature": 0.8,
        "top_p": 0.95,
        "top_k": 50,
        "repetition_penalty": 1.2,
        "no_repeat_ngram_size": 3
    }
}